{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "E7CW4zPsZM_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-terrier\n",
        "!pip install nltk\n"
      ],
      "metadata": {
        "id": "EZKx0XwuUQYG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73f4a9e9-6efe-4662-81e9-1b44fd3ce136"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-terrier\n",
            "  Downloading python-terrier-0.10.1.tar.gz (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m891.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from python-terrier) (2.0.3)\n",
            "Collecting wget (from python-terrier)\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from python-terrier) (4.66.4)\n",
            "Collecting pyjnius>=1.4.2 (from python-terrier)\n",
            "  Downloading pyjnius-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting matchpy (from python-terrier)\n",
            "  Downloading matchpy-0.5.5-py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.6/69.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deprecated (from python-terrier)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting chest (from python-terrier)\n",
            "  Downloading chest-0.2.3.tar.gz (9.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.11.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from python-terrier) (2.31.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.4.2)\n",
            "Collecting nptyping==1.4.4 (from python-terrier)\n",
            "  Downloading nptyping-1.4.4-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: more_itertools in /usr/local/lib/python3.10/dist-packages (from python-terrier) (10.1.0)\n",
            "Collecting ir_datasets>=0.3.2 (from python-terrier)\n",
            "  Downloading ir_datasets-0.5.7-py3-none-any.whl (337 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.9/337.9 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (3.1.4)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.14.2)\n",
            "Collecting ir_measures>=0.3.1 (from python-terrier)\n",
            "  Downloading ir_measures-0.3.3.tar.gz (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.8/48.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting dill (from python-terrier)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytrec_eval_terrier>=0.5.3 (from python-terrier)\n",
            "  Downloading pytrec_eval_terrier-0.5.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.4/287.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typish>=1.7.0 (from nptyping==1.4.4->python-terrier)\n",
            "  Downloading typish-1.9.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from ir_datasets>=0.3.2->python-terrier) (4.12.3)\n",
            "Collecting inscriptis>=2.2.0 (from ir_datasets>=0.3.2->python-terrier)\n",
            "  Downloading inscriptis-2.5.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.5.2 in /usr/local/lib/python3.10/dist-packages (from ir_datasets>=0.3.2->python-terrier) (4.9.4)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ir_datasets>=0.3.2->python-terrier) (6.0.1)\n",
            "Collecting trec-car-tools>=2.5.4 (from ir_datasets>=0.3.2->python-terrier)\n",
            "  Downloading trec_car_tools-2.6-py3-none-any.whl (8.4 kB)\n",
            "Collecting lz4>=3.1.10 (from ir_datasets>=0.3.2->python-terrier)\n",
            "  Downloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting warc3-wet>=0.2.3 (from ir_datasets>=0.3.2->python-terrier)\n",
            "  Downloading warc3_wet-0.2.3-py3-none-any.whl (13 kB)\n",
            "Collecting warc3-wet-clueweb09>=0.2.5 (from ir_datasets>=0.3.2->python-terrier)\n",
            "  Downloading warc3-wet-clueweb09-0.2.5.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting zlib-state>=0.1.3 (from ir_datasets>=0.3.2->python-terrier)\n",
            "  Downloading zlib-state-0.1.6.tar.gz (9.5 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ijson>=3.1.3 (from ir_datasets>=0.3.2->python-terrier)\n",
            "  Downloading ijson-3.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (111 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.8/111.8 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unlzw3>=0.2.1 (from ir_datasets>=0.3.2->python-terrier)\n",
            "  Downloading unlzw3-0.2.2-py3-none-any.whl (6.1 kB)\n",
            "Collecting cwl-eval>=1.0.10 (from ir_measures>=0.3.1->python-terrier)\n",
            "  Downloading cwl-eval-1.0.12.tar.gz (31 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->python-terrier) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->python-terrier) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->python-terrier) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->python-terrier) (2024.2.2)\n",
            "Collecting heapdict (from chest->python-terrier)\n",
            "  Downloading HeapDict-1.0.1-py3-none-any.whl (3.9 kB)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated->python-terrier) (1.14.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->python-terrier) (2.1.5)\n",
            "Collecting multiset<3.0,>=2.0 (from matchpy->python-terrier)\n",
            "  Downloading multiset-2.1.1-py2.py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->python-terrier) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->python-terrier) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->python-terrier) (2024.1)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels->python-terrier) (0.5.6)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels->python-terrier) (24.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.4.1->ir_datasets>=0.3.2->python-terrier) (2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.6->statsmodels->python-terrier) (1.16.0)\n",
            "Collecting cbor>=1.0.0 (from trec-car-tools>=2.5.4->ir_datasets>=0.3.2->python-terrier)\n",
            "  Downloading cbor-1.0.0.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: python-terrier, ir_measures, chest, wget, cwl-eval, warc3-wet-clueweb09, zlib-state, cbor\n",
            "  Building wheel for python-terrier (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-terrier: filename=python_terrier-0.10.1-py3-none-any.whl size=117255 sha256=115dc635ded03f476ce1fc79bd530e59ffd43c8903974738ccf9a60629dec999\n",
            "  Stored in directory: /root/.cache/pip/wheels/58/88/6d/fd8acd9b71f17907235017371cff103c736d40e55101783cc7\n",
            "  Building wheel for ir_measures (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ir_measures: filename=ir_measures-0.3.3-py3-none-any.whl size=61183 sha256=29af9fc2ef48652498efee6f71443993d8f8257b3c799cd15ce565f725c38777\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/0e/22/718279f23fef1673a4c5e433881c25080a6afaa147e007183e\n",
            "  Building wheel for chest (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chest: filename=chest-0.2.3-py3-none-any.whl size=7612 sha256=19c5afcd9a5984e8beef270106d45a5354e18a5032f11c6914b335f49440e6cd\n",
            "  Stored in directory: /root/.cache/pip/wheels/88/cf/99/4773b31f855f9ecedc32a0ae400f7a4a3001b37c439b6d1a73\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=dd7082642cba3cc47e54f4a226c418a97b71d4cdc9c6f6d5d8c032e95190307a\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "  Building wheel for cwl-eval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cwl-eval: filename=cwl_eval-1.0.12-py3-none-any.whl size=38068 sha256=b59504a11fb12fc6a6dc76fb533340c9aa18813e39687bf82649f2bee2c4a998\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/c1/94/94a3e5379b1aa8fb7c7f1ad1956305d5edc98ef745b6067d87\n",
            "  Building wheel for warc3-wet-clueweb09 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for warc3-wet-clueweb09: filename=warc3_wet_clueweb09-0.2.5-py3-none-any.whl size=18919 sha256=9c63608a653340948c41c14288bb8009236002578f38ef74243a1ba63776bdd7\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/d7/91/7ffb991df87e62355d945745035470ba2616aa3d83a250b5f9\n",
            "  Building wheel for zlib-state (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for zlib-state: filename=zlib_state-0.1.6-cp310-cp310-linux_x86_64.whl size=21162 sha256=a8a1ee845b0ad966738e8e3ac96a1caadfdcc0fa05fd0411054abb1a664abb5c\n",
            "  Stored in directory: /root/.cache/pip/wheels/32/72/7e/aff80f26e926b6e1fb08dfb52aba03c0e058f5e2258deb50a9\n",
            "  Building wheel for cbor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cbor: filename=cbor-1.0.0-cp310-cp310-linux_x86_64.whl size=53430 sha256=c1fdc84cc73c33fad8796e93f8180a07c79ee4772cc7336d93628e9df188b080\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/df/c9/b39e40eccaf76dbd218556639a6dc81562226f4c6a64902c85\n",
            "Successfully built python-terrier ir_measures chest wget cwl-eval warc3-wet-clueweb09 zlib-state cbor\n",
            "Installing collected packages: wget, warc3-wet-clueweb09, warc3-wet, typish, pyjnius, multiset, ijson, heapdict, cbor, zlib-state, unlzw3, trec-car-tools, pytrec_eval_terrier, nptyping, matchpy, lz4, dill, deprecated, cwl-eval, chest, ir_measures, inscriptis, ir_datasets, python-terrier\n",
            "Successfully installed cbor-1.0.0 chest-0.2.3 cwl-eval-1.0.12 deprecated-1.2.14 dill-0.3.8 heapdict-1.0.1 ijson-3.2.3 inscriptis-2.5.0 ir_datasets-0.5.7 ir_measures-0.3.3 lz4-4.3.3 matchpy-0.5.5 multiset-2.1.1 nptyping-1.4.4 pyjnius-1.6.1 python-terrier-0.10.1 pytrec_eval_terrier-0.5.6 trec-car-tools-2.6 typish-1.9.3 unlzw3-0.2.2 warc3-wet-0.2.3 warc3-wet-clueweb09-0.2.5 wget-3.2 zlib-state-0.1.6\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVeF6crveQlN",
        "outputId": "91f89793-b40e-4513-ec07-eb7fa23fc937"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.34.0-py2.py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.3.3)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<2,>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.25.2)\n",
            "Requirement already satisfied: packaging<25,>=16.8 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.0)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.0.3)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (14.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.31.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.1)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.3.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.11.0)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Collecting watchdog>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-4.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.2.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.18.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
            "Installing collected packages: watchdog, smmap, pydeck, gitdb, gitpython, streamlit\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.43 pydeck-0.9.1 smmap-5.0.1 streamlit-1.34.0 watchdog-4.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJ2236gXEnFI"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "import math\n",
        "from nltk.stem import *\n",
        "from nltk.stem.porter import *\n",
        "from collections import Counter\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.nn.functional import softmax\n",
        "pd.set_option('display.max_colwidth', 150)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Collection"
      ],
      "metadata": {
        "id": "gtFucM5fUYhi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# drive.mount('/content/drive')\n",
        "# file_path = '/content/drive/MyDrive/Corona_NLP_test.csv'\n",
        "df = pd.read_csv(\"/content/My_own_data (2).csv\")"
      ],
      "metadata": {
        "id": "sagTeAxpFk5N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "72f5943e-876c-4fc2-e56a-8cb183ae969d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/My_own_data (2).csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-5f2791aa694d>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# drive.mount('/content/drive')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# file_path = '/content/drive/MyDrive/Corona_NLP_test.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/My_own_data (2).csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/My_own_data (2).csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "F2X1rCpOGnt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"docno\"]=df[\"document_id\"].astype(str)\n",
        "df.drop(columns = [\"document_id\"], inplace = True)"
      ],
      "metadata": {
        "id": "Us-2Rx4yVteb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "rKYv3DdDaaPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing"
      ],
      "metadata": {
        "id": "ka7Ed3eMUpVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyterrier as pt\n",
        "if not pt.started():\n",
        "  pt.init(boot_packages=[\"com.github.terrierteam:terrier-prf:-SNAPSHOT\"])"
      ],
      "metadata": {
        "id": "V5GN9c87GoZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "LW8IQSSeUxPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "WgSflCQzVAPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "Uh_kBYpHVCDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def Steem_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "    return ' '.join(stemmed_tokens)\n",
        "def remove_stopwords(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    filtered_tokens = [word.lower() for word in tokens if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_tokens)\n",
        "def clean(sentence):\n",
        "   tokens = word_tokenize(sentence)\n",
        "   cleaned = []\n",
        "   for text in tokens:\n",
        "\n",
        "    text = re.sub(r\"http\\S+\", \" \", text)\n",
        "    text = re.sub(r'\\([^)]*\\)', '', text)\n",
        "    text = re.sub(r\"RT \", \" \", text)\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "    text = re.sub(r\"@[\\w]*\", \" \", text)\n",
        "    text = re.sub(r\"[\\.\\,\\#\\(\\)_\\|\\:\\?\\?!&\\-\\$;'/\\=]\", \" \", text)\n",
        "    text = re.sub(r'\\t', ' ', text)\n",
        "    text = re.sub(r'\\n', ' ', text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    cleaned.append(text)\n",
        "\n",
        "   return ' '.join(cleaned)\n",
        "def Preprocess(df, col_name):\n",
        "  df['processed'] = df[col_name].apply(clean)\n",
        "  df['processed'] = df['processed'].apply(remove_stopwords)\n",
        "  df['processed'] = df['processed'].apply(Steem_text)\n",
        "def find_duplicates(lst):\n",
        "    counts = Counter(lst)\n",
        "    duplicates = [item for item, count in counts.items() if count > 1]\n",
        "    return duplicates\n",
        "def remove_extra_spaces(text):\n",
        "  words = text.split()\n",
        "  return \" \".join(words)"
      ],
      "metadata": {
        "id": "DgmRXKsqVTw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Preprocess(df, \"content\")\n"
      ],
      "metadata": {
        "id": "rg7vKwOSXJo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"content\"] = df[\"content\"].apply(clean)\n",
        "df[\"content\"] = df[\"content\"].apply(remove_extra_spaces)"
      ],
      "metadata": {
        "id": "FGkwtRdzrYhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "9dV553a0W85X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Indexing"
      ],
      "metadata": {
        "id": "MXBxgjSqZmAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "indexer = pt.DFIndexer(\"./DatasetIndex\", overwrite=True)\n",
        "index_ref = indexer.index(df[\"processed\"], df[\"docno\"])\n",
        "index_ref.toString()\n",
        "index = pt.IndexFactory.of(index_ref)"
      ],
      "metadata": {
        "id": "vD1H7FwoV3i3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = pt.IndexFactory.of(\"/content/DatasetIndex/data.properties\")"
      ],
      "metadata": {
        "id": "11XE2PlomFem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_index = index.getDocumentIndex()\n"
      ],
      "metadata": {
        "id": "HKr_e65t-hmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(index.getCollectionStatistics())\n"
      ],
      "metadata": {
        "id": "bn80wh0WBfan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_Terms = []\n",
        "for kv in index.getLexicon():\n",
        "  unique_Terms.append(kv.getKey())\n",
        "print(unique_Terms)\n"
      ],
      "metadata": {
        "id": "PrV6IP7UBQIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''loops over the postings of each terms get the id and the freq\n",
        "and append to sub_posting then append to the posting'''\n",
        "''' For each term, maintain a list of document IDs where the term appears along\n",
        "with the frequency of occurrence.'''\n",
        "''' list with each document and the freq of the word inside it for each word'''\n",
        "mapping = {}\n",
        "for kv in index.getLexicon():\n",
        "       post = []\n",
        "       for j in index.getInvertedIndex().getPostings(index.getLexicon()[kv.key]):\n",
        "          sub_post = []\n",
        "          sub_post.append(j.getId())\n",
        "          sub_post.append(j.getFrequency())\n",
        "          post.append(sub_post)\n",
        "       mapping[kv.key] = post\n",
        "print(mapping[\"covid\"])"
      ],
      "metadata": {
        "id": "IddgSikkCXbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query Processing"
      ],
      "metadata": {
        "id": "-w8koj2x5DAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Preprocess_Query(Query):\n",
        "  Query = clean(Query)\n",
        "  Query = remove_stopwords(Query)\n",
        "  Query = Steem_text(Query)\n",
        "  return Query"
      ],
      "metadata": {
        "id": "mYkTblt0AKZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = Preprocess_Query(\"Egypt country\")\n",
        "tokenized = word_tokenize(query)\n",
        "documents = []\n",
        "common = []\n",
        "for word in tokenized:\n",
        " if word in unique_Terms:\n",
        "  for j in index.getInvertedIndex().getPostings(index.getLexicon()[word]):\n",
        "\n",
        "    documents.append(j.getId())\n",
        "common = find_duplicates(documents)\n",
        "''' common calles the find_duplicates function to search for the common documents between the ids\n",
        "in the documents list and this will be the list that have all the terms'''"
      ],
      "metadata": {
        "id": "GSFKrvVq_8pP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Identify relevant documents by leveraging the inverted index'''\n",
        "print(\"\\033[1mAll Relevant Documents\\033[0m\\n\")\n",
        "for i in range(0,5):\n",
        "  print(df[\"content\"][documents[i]])\n",
        "  print(df[\"links\"][documents[i]])\n",
        "  print(\"\\n\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BJ60en08EPf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''''Retrieve documents that contain all the terms from the query.'''\n",
        "print(\"\\033[1mDouments containing all terms of the query\\033[0m\\n\")\n",
        "for i in range(0,5):\n",
        "  print(df[\"content\"][common[i]])\n",
        "  print(df[\"links\"][common[i]])\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "id": "vcUJ-iZGELeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF function"
      ],
      "metadata": {
        "id": "9NbPtyz0OUsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Rank the retrieved documents based ranking algorithm (TF-IDF).'''"
      ],
      "metadata": {
        "id": "TBAl28C_SEp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_retr = pt.BatchRetrieve(index, controls = {\"wmodel\": \"TF_IDF\"},num_results=10)\n"
      ],
      "metadata": {
        "id": "yRKUIKk4670s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = Preprocess_Query(\"Egypt country\")\n",
        "results=tfidf_retr.search(query)\n",
        "results"
      ],
      "metadata": {
        "id": "__cNILrw5gpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents_rank = {}\n",
        "for i in range(len(results)):\n",
        "  documents_rank[results[\"docno\"][i]] = [results[\"docid\"][i],results[\"rank\"][i]]\n",
        "print(documents_rank)"
      ],
      "metadata": {
        "id": "V9hPnhJm9joo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\033[1mTop 10 retrieved documents with ranking\\033[0m\\n\")\n",
        "for key, value in documents_rank.items():\n",
        "    print(df[\"content\"][value[0]])\n",
        "    print(df[\"links\"][value[0]])\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "xIZtN9ER-nwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query Expansion"
      ],
      "metadata": {
        "id": "Oas6ob0BIEw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = Preprocess_Query(\"Covid\")\n",
        "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\",num_results=10)\n",
        "results = bm25.search(query)\n",
        "results\n"
      ],
      "metadata": {
        "id": "tpJMYcS0GCrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rm3_expander = pt.rewrite.RM3(index,fb_terms=10, fb_docs=100)\n",
        "rm3_qe = bm25 >> rm3_expander\n",
        "expanded_query = rm3_qe.search(\"Covid\")\n",
        "res = []\n",
        "retrieved = []\n",
        "if (len(expanded_query) !=0):\n",
        "  expanded_query_formatted = ' '.join(expanded_query.iloc[0][\"query\"].split()[1:])\n",
        "  results_wqe = bm25.search(expanded_query_formatted)\n",
        "  indecies = results_wqe['docno'].loc[0:10].tolist()\n",
        "  Better_Queries = df[['content', 'links', 'docno']]\n",
        "\n",
        "\n",
        "  for i in range(len(indecies)):\n",
        "      res.append(Better_Queries[\"links\"][int(indecies[i])-1] + \" \" + Better_Queries[\"content\"][int(indecies[i])-1][0:248])\n",
        "      retrieved.append(Better_Queries[\"docno\"][int(indecies[i])-1])\n",
        "\n",
        "else:\n",
        "  print(\"No results found\")\n"
      ],
      "metadata": {
        "id": "aY6RY90RIh2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_engine(query):\n",
        "    query = Preprocess_Query(query)\n",
        "    bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\",num_results=10)\n",
        "    rm3_expander = pt.rewrite.RM3(index,fb_terms=10, fb_docs=100)\n",
        "    rm3_qe = bm25 >> rm3_expander\n",
        "    expanded_query = rm3_qe.search(query)\n",
        "\n",
        "    res = []\n",
        "    retrieved = []\n",
        "    if (len(expanded_query) !=0):\n",
        "      expanded_query_formatted = ' '.join(expanded_query.iloc[0][\"query\"].split()[1:])\n",
        "      results_wqe = bm25.search(expanded_query_formatted)\n",
        "      indecies = results_wqe['docno'].loc[0:10].tolist()\n",
        "      Better_Queries = df[['content', 'links', 'docno']]\n",
        "\n",
        "\n",
        "      for i in range(len(indecies)):\n",
        "          res.append(Better_Queries[\"links\"][int(indecies[i])-1] + \" \" + Better_Queries[\"content\"][int(indecies[i])-1][0:248])\n",
        "          retrieved.append(Better_Queries[\"docno\"][int(indecies[i])-1])\n",
        "\n",
        "    else:\n",
        "      print(\"No results found\")\n",
        "\n",
        "\n",
        "    return res"
      ],
      "metadata": {
        "id": "cSVJ07ZSe5Tb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display(query):\n",
        "    query = Preprocess_Query(query)\n",
        "    bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\",num_results=10)\n",
        "    rm3_expander = pt.rewrite.RM3(index,fb_terms=10, fb_docs=100)\n",
        "    rm3_qe = bm25 >> rm3_expander\n",
        "    expanded_query = rm3_qe.search(query)\n",
        "    res = []\n",
        "    retrieved = []\n",
        "    if (len(expanded_query) !=0):\n",
        "      expanded_query_formatted = ' '.join(expanded_query.iloc[0][\"query\"].split()[1:])\n",
        "      results_wqe = bm25.search(expanded_query_formatted)\n",
        "      indecies = results_wqe['docno'].loc[0:10].tolist()\n",
        "      Better_Queries = df[['content', 'links', 'docno']]\n",
        "\n",
        "\n",
        "      for i in range(len(indecies)):\n",
        "          res.append(Better_Queries[\"links\"][int(indecies[i])-1] + \" \" + Better_Queries[\"content\"][int(indecies[i])-1][0:248])\n",
        "          retrieved.append(Better_Queries[\"docno\"][int(indecies[i])-1])\n",
        "\n",
        "    else:\n",
        "      print(\"No results found\")\n",
        "      print(query)\n",
        "\n",
        "\n",
        "    return [res, retrieved]"
      ],
      "metadata": {
        "id": "6CqjGDP1Lu-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate"
      ],
      "metadata": {
        "id": "4z0T2UlNKUt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(\"Covid\")"
      ],
      "metadata": {
        "id": "a12EIsmVyz8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_queries = [\"Standup Comedy\", \"Gaza\",\"Egypt\", \"Zewail City\", \"Covid\"]\n",
        "relevant_docs = {\n",
        "    'Standup Comedy': [526, 527, 431, 432, 433, 434, 435],\n",
        "    'Gaza': [381, 382, 383, 384, 385, 528, 529, 530, 531],\n",
        "    'Egypt': [366, 367, 368, 369, 370, 532, 533, 534],\n",
        "    'Zewail City': [376, 377, 378, 379, 380, 535, 536],\n",
        "    'Covid' :[436, 437, 438, 439, 440, 537, 538, 539]\n",
        "}\n",
        "\n",
        "retrieved_docs = {}\n",
        "\n",
        "for q in test_queries:\n",
        "    docnos = display(q)[1]\n",
        "    retrieved_docs[q] = [int(doc) for doc in docnos]\n",
        "\n",
        "\n",
        "print(retrieved_docs)"
      ],
      "metadata": {
        "id": "l_Ou8a0QLoFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(qrels, retrieved_docs):\n",
        "    precision = {}\n",
        "    recall = {}\n",
        "    f1 = {}\n",
        "    ndcg = {}\n",
        "\n",
        "    for query in test_queries:\n",
        "        relevant = set(qrels[query])\n",
        "        retrieved = set(retrieved_docs[query])\n",
        "\n",
        "        dcg = sum(1 / (math.log2(i + 2)) for i, doc in enumerate(retrieved) if doc in relevant)\n",
        "        idcg = sum(1 / (math.log2(i + 2)) for i in range(min(len(relevant), len(retrieved))))\n",
        "        ndcg[query] = dcg / idcg if idcg > 0 else 0\n",
        "\n",
        "    return ndcg\n",
        "\n",
        "\n",
        "ndcg = calculate_metrics(relevant_docs, retrieved_docs)\n",
        "\n",
        "# Print metrics\n",
        "for query in test_queries:\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"NDCG: {ndcg[query]:.4f}\")\n"
      ],
      "metadata": {
        "id": "urvVdODfVJ-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GUI"
      ],
      "metadata": {
        "id": "llBKDWB4fCqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "from nltk.stem import *\n",
        "from nltk.stem.porter import *\n",
        "from collections import Counter\n",
        "import time\n",
        "import math\n",
        "df = pd.read_csv(\"/content/My_own_data (2).csv\")\n",
        "df[\"docno\"]=df[\"document_id\"].astype(str)\n",
        "df.drop(columns = [\"document_id\"], inplace = True)\n",
        "df.dropna(inplace=True)\n",
        "import pyterrier as pt\n",
        "if not pt.started():\n",
        "  pt.init(boot_packages=[\"com.github.terrierteam:terrier-prf:-SNAPSHOT\"])\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def Steem_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "    return ' '.join(stemmed_tokens)\n",
        "def remove_stopwords(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    filtered_tokens = [word.lower() for word in tokens if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_tokens)\n",
        "def clean(sentence):\n",
        "   tokens = word_tokenize(sentence)\n",
        "   cleaned = []\n",
        "   for text in tokens:\n",
        "\n",
        "    text = re.sub(r\"http\\S+\", \" \", text)\n",
        "    text = re.sub(r'\\([^)]*\\)', '', text)\n",
        "    text = re.sub(r\"RT \", \" \", text)\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "    text = re.sub(r\"@[\\w]*\", \" \", text)\n",
        "    text = re.sub(r\"[\\.\\,\\#\\(\\)_\\|\\:\\?\\?!&\\-\\$;'/\\=]\", \" \", text)\n",
        "    text = re.sub(r'\\t', ' ', text)\n",
        "    text = re.sub(r'\\n', ' ', text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    cleaned.append(text)\n",
        "\n",
        "   return ' '.join(cleaned)\n",
        "def Preprocess(df, col_name):\n",
        "  df['processed'] = df[col_name].apply(clean)\n",
        "  df['processed'] = df['processed'].apply(remove_stopwords)\n",
        "  df['processed'] = df['processed'].apply(Steem_text)\n",
        "def find_duplicates(lst):\n",
        "    counts = Counter(lst)\n",
        "    duplicates = [item for item, count in counts.items() if count > 1]\n",
        "    return duplicates\n",
        "def remove_extra_spaces(text):\n",
        "  words = text.split()\n",
        "  return \" \".join(words)\n",
        "def Preprocess_Query(Query):\n",
        "  Query = clean(Query)\n",
        "  Query = remove_stopwords(Query)\n",
        "  Query = Steem_text(Query)\n",
        "  return Query\n",
        "index = pt.IndexFactory.of(\"/content/DatasetIndex/data.properties\")\n",
        "def search(query):\n",
        "    start_time = time.time()  # Record the start time\n",
        "    query = Preprocess_Query(query)\n",
        "    bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\", num_results=10)\n",
        "    rm3_expander = pt.rewrite.RM3(index, fb_terms=10, fb_docs=100)\n",
        "    rm3_qe = bm25 >> rm3_expander\n",
        "    expanded_query = rm3_qe.search(query)\n",
        "    res = []\n",
        "    retrieved = []\n",
        "    if (len(expanded_query) != 0):\n",
        "        expanded_query_formatted = ' '.join(expanded_query.iloc[0][\"query\"].split()[1:])\n",
        "        results_wqe = bm25.search(expanded_query_formatted)\n",
        "        indecies = results_wqe['docno'].loc[0:10].tolist()\n",
        "        Better_Queries = df[['content', 'links', 'docno']]\n",
        "\n",
        "        for i in range(len(indecies)):\n",
        "            res.append(Better_Queries[\"links\"][int(indecies[i]) - 1] + \" \" + Better_Queries[\"content\"][int(indecies[i]) - 1][0:248])\n",
        "            retrieved.append(Better_Queries[\"docno\"][int(indecies[i]) - 1])\n",
        "\n",
        "    else:\n",
        "        res.append(\"No Results Found\")\n",
        "\n",
        "    end_time = time.time()  # Record the end time\n",
        "    search_time = end_time - start_time  # Calculate the search time\n",
        "    return res, search_time, retrieved\n",
        "def display(query):\n",
        "    query = Preprocess_Query(query)\n",
        "    bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\",num_results=10)\n",
        "    rm3_expander = pt.rewrite.RM3(index,fb_terms=10, fb_docs=100)\n",
        "    rm3_qe = bm25 >> rm3_expander\n",
        "    expanded_query = rm3_qe.search(query)\n",
        "    res = []\n",
        "    retrieved = []\n",
        "    if (len(expanded_query) !=0):\n",
        "      expanded_query_formatted = ' '.join(expanded_query.iloc[0][\"query\"].split()[1:])\n",
        "      results_wqe = bm25.search(expanded_query_formatted)\n",
        "      indecies = results_wqe['docno'].loc[0:10].tolist()\n",
        "      Better_Queries = df[['content', 'links', 'docno']]\n",
        "\n",
        "\n",
        "      for i in range(len(indecies)):\n",
        "          res.append(Better_Queries[\"links\"][int(indecies[i])-1] + \" \" + Better_Queries[\"content\"][int(indecies[i])-1][0:248])\n",
        "          retrieved.append(Better_Queries[\"docno\"][int(indecies[i])-1])\n",
        "\n",
        "    else:\n",
        "      print(\"No results found\")\n",
        "      print(query)\n",
        "\n",
        "\n",
        "    return [res, retrieved]\n",
        "test_queries = [\"Standup Comedy\", \"Gaza\",\"Egypt\", \"Zewail City\", \"Covid\"]\n",
        "relevant_docs = {\n",
        "    'Standup Comedy': [526, 527, 431, 432, 433, 434, 435],\n",
        "    'Gaza': [381, 382, 383, 384, 385, 528, 529, 530, 531],\n",
        "    'Egypt': [366, 367, 368, 369, 370, 532, 533, 534],\n",
        "    'Zewail City': [376, 377, 378, 379, 380, 535, 536],\n",
        "    'Covid' :[436, 437, 438, 439, 440, 537, 538, 539]\n",
        "}\n",
        "\n",
        "retrieved_docs = {}\n",
        "\n",
        "for q in test_queries:\n",
        "    docnos = display(q)[1]\n",
        "    retrieved_docs[q] = [int(doc) for doc in docnos]\n",
        "def calculate_metrics(qrels, retrieved_docs):\n",
        "    precision = {}\n",
        "    recall = {}\n",
        "    f1 = {}\n",
        "    ndcg = {}\n",
        "    for query in test_queries:\n",
        "        relevant = set(qrels[query])\n",
        "        retrieved = set(retrieved_docs[query])\n",
        "        dcg = sum(1 / (math.log2(i + 2)) for i, doc in enumerate(retrieved) if doc in relevant)\n",
        "        idcg = sum(1 / (math.log2(i + 2)) for i in range(min(len(relevant), len(retrieved))))\n",
        "        ndcg[query] = dcg / idcg if idcg > 0 else 0\n",
        "\n",
        "    return ndcg\n",
        "def main():\n",
        "    st.title(\"Doodle\")\n",
        "    st.text(\"Enter a search query\")\n",
        "\n",
        "    search_query = st.text_input(\"Search query:\")\n",
        "    submit = st.button(\"Search\")\n",
        "\n",
        "    if submit and search_query:\n",
        "      search_results, search_time, retrieved = search(search_query)\n",
        "      if (search_query in test_queries):\n",
        "\n",
        "          ndcg = calculate_metrics(relevant_docs, retrieved_docs)\n",
        "\n",
        "          st.write(\"Search Time:\", round(search_time, 2), \"seconds\")\n",
        "          st.write(\"Efficiency:\", round(ndcg[search_query], 2)*100, \"%\")\n",
        "          st.write(\"Top Ten Results\")\n",
        "          for result in search_results:\n",
        "            st.write(result)\n",
        "      else:\n",
        "        st.write(\"Search Time:\", round(search_time, 2), \"seconds\")\n",
        "\n",
        "        st.write(\"Top Ten Results\")\n",
        "        for result in search_results:\n",
        "          st.write(result)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "CDCaY6_vWEOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run main.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "79170PTpen7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ri5irOmxPkeg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}